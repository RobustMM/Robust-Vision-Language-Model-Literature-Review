# Robust-Vision-Language-Model-Literature-Review
<!-- <h1>Vision-Language Foundation Model</h1> -->
<details>
<summary>Vision-Language Foundation Model</summary>

## Vision-Language Model - Vision-Language Foundation Model
| Year | Venue | Title | Institute | Code |
| :---:| :---: | :---: | :---: | :---: |
| 2023 | Arxiv | [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) | Salesforce Research | [Link](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)
| 2023 | Arxiv | [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500) | Salesforce Research | [Link](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)
| 2023 | CVPR | [Image as a Foreign Language: BEIT Pretraining for Vision and Vision-Language Tasks](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Image_as_a_Foreign_Language_BEiT_Pretraining_for_Vision_and_CVPR_2023_paper.pdf) | Microsoft Corporation| [Link](https://github.com/microsoft/unilm/tree/master/beit3)
| 2023 | ICCV | [ALIP: Adaptive Language-Image Pre-training with Synthetic Caption](https://arxiv.org/abs/2308.08428) | DeepGlint | [Link](https://github.com/deepglint/ALIP)
| 2023 | ICCV | [GrowCLIP: Data-aware Automatic Model Growing for Large-scale Contrastive Language-Image Pre-training](https://openaccess.thecvf.com/content/ICCV2023/papers/Deng_GrowCLIP_Data-Aware_Automatic_Model_Growing_for_Large-scale_Contrastive_Language-Image_Pre-Training_ICCV_2023_paper.pdf) | Sun Yat-sen University  | -
| 2023 | ICLR | [PaLI: A Jointly-Scaled Multilingual Language-Image Model](https://openreview.net/forum?id=mWVoBz4W0u) | Google Research | [Link](https://github.com/kyegomez/PALI)
| 2023 | ICML | [Scaling Vision Transformers to 22 Billion Parameters](https://openreview.net/pdf?id=Lhyy8H75KA) | Google Research | -
| 2022 | ICLR | [SimVLM: Simple Visual Language Model Pretraining with Weak Supervision](https://openreview.net/pdf?id=GUrhfTuf_3) | Google Research | -
| 2022 | ICLR | [Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm](https://arxiv.org/abs/2110.05208) | SenseTime Research | [Link](https://github.com/Sense-GVT/DeCLIP)
| 2022 | ICML  | [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://proceedings.mlr.press/v162/li22n/li22n.pdf)  | Salesforce Research | [Link](https://github.com/salesforce/BLIP)
| 2021 | ICML  | [Learning Transferable Visual Models From Natural Language Supervision](https://proceedings.mlr.press/v139/radford21a/radford21a.pdf) | OpenAI | [Link](https://github.com/OpenAI/CLIP)
| 2021 | ICML | [Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://proceedings.mlr.press/v139/jia21b/jia21b.pdf) | Google Research| -
| 2021 | NeurIPS | [Align before Fuse: Vision and Language Representation Learning with Momentum Distillation](https://proceedings.neurips.cc/paper/2021/file/505259756244493872b7709a8a01b536-Paper.pdf) | Salesforce Research | [Link](https://github.com/salesforce/ALBEF)
</details>

<details> <summary>Vision-Language Model - Domain Generalization</summary>

## Vision-Language Model - Domain Generalization
| Year | Venue | Title | Institute | Code |
| :---:| :---: | :---: | :---: | :---: |
| 2023 | CVPR | [Improving Zero-shot Generalization and Robustness of Multi-modal Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Ge_Improving_Zero-Shot_Generalization_and_Robustness_of_Multi-Modal_Models_CVPR_2023_paper.pdf) | Google Research | [Link](https://github.com/gyhandy/Hierarchy-CLIP) |
| 2023 | CVPR | [MaPLe: Multi-modal Prompt Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Khattak_MaPLe_Multi-Modal_Prompt_Learning_CVPR_2023_paper.pdf) | Mohamed bin Zayed University of AI | [Link](https://github.com/muzairkhattak/multimodal-prompt-learning) |
| 2023 | CVPR | [Task Residual for Tuning Vision-Language Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Task_Residual_for_Tuning_Vision-Language_Models_CVPR_2023_paper.pdf) | National University of Singapore | [Link](https://github.com/geekyutao/TaskRes) |
| 2023 | CVPR | [Visual-Language Prompt Tuning with Knowledge-guided Context Optimization](https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_Visual-Language_Prompt_Tuning_With_Knowledge-Guided_Context_Optimization_CVPR_2023_paper.pdf) | Chinese Academy of Sciences | [Link](https://github.com/htyao89/KgCoOp) |
| 2023 | ICCV | [Bayesian Prompt Learning for Image-Language Model Generalization](https://openaccess.thecvf.com/content/ICCV2023/papers/Derakhshani_Bayesian_Prompt_Learning_for_Image-Language_Model_Generalization_ICCV_2023_paper.pdf) | University of Amsterdam | [Link](https://github.com/saic-fi/Bayesian-Prompt-Learning) |
| 2023 | ICCV | [Black Box Few-Shot Adaptation for Vision-Language models](https://openaccess.thecvf.com/content/ICCV2023/papers/Ouali_Black_Box_Few-Shot_Adaptation_for_Vision-Language_Models_ICCV_2023_paper.pdf) | Samsung AI Cambridge | [Link](https://github.com/saic-fi/LFA) |
| 2023 | ICCV | [Distribution-Aware Prompt Tuning for Vision-Language Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Cho_Distribution-Aware_Prompt_Tuning_for_Vision-Language_Models_ICCV_2023_paper.pdf) | Korea University | [Link](https://github.com/mlvlab/DAPT) |
| 2023 | ICCV | [Gradient-Regulated Meta-Prompt Learning for Generalizable Vision-Language Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Gradient-Regulated_Meta-Prompt_Learning_for_Generalizable_Vision-Language_Models_ICCV_2023_paper.pdf) | Zhejiang University | - |
| 2023 | ICCV | [Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Kan_Knowledge-Aware_Prompt_Tuning_for_Generalizable_Vision-Language_Models_ICCV_2023_paper.pdf) | Qilu University of Technology | - |
| 2023 | ICCV | [LoGoPrompt: Synthetic Text Images Can Be Good Visual Prompts for Vision-Language Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Shi_LoGoPrompt_Synthetic_Text_Images_Can_Be_Good_Visual_Prompts_for_ICCV_2023_paper.pdf) | ShanghaiTech University | - |
| 2023 | ICCV | [PADCLIP: Pseudo-labeling with Adaptive Debiasing in CLIP for Unsupervised Domain Adaptation](https://openaccess.thecvf.com/content/ICCV2023/papers/Lai_PADCLIP_Pseudo-labeling_with_Adaptive_Debiasing_in_CLIP_for_Unsupervised_Domain_ICCV_2023_paper.pdf) | University of California, Davis | - |
| 2023 | ICCV | [PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization](https://openaccess.thecvf.com/content/ICCV2023/papers/Cho_PromptStyler_Prompt-driven_Style_Generation_for_Source-free_Domain_Generalization_ICCV_2023_paper.pdf) | ADD | - |
| 2023 | ICCV | [Read-only Prompt Optimization for Vision-Language Few-shot Learning](https://openaccess.thecvf.com//content/ICCV2023/papers/Lee_Read-only_Prompt_Optimization_for_Vision-Language_Few-shot_Learning_ICCV_2023_paper.pdf) | Korea University | [Link](https://github.com/mlvlab/RPO) |
| 2023 | ICCV | [Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-trained Vision-Language Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Zheng_Regularized_Mask_Tuning_Uncovering_Hidden_Knowledge_in_Pre-Trained_Vision-Language_Models_ICCV_2023_paper.pdf) | Zhejiang University | [Link](https://github.com/wuw2019/R-AMT) |
| 2023 | ICCVW | [AD-CLIP: Adapting Domains in Prompt Space Using CLIP](https://arxiv.org/abs/2308.05659) | Indian Institute of Technology Bombay | - |
| 2023 | ICLR | [PLOT: Prompt Learning with Optimal Transport for Vision-Language Models](https://arxiv.org/abs/2210.01253) | Carnegie Mellon University | [Link](https://github.com/CHENGY12/PLOT) |
| 2022 | CVPR | [Conditional Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2203.05557) | Nanyang Technological University | [Link](https://github.com/KaiyangZhou/CoOp) |
| 2022 | CVPR | [Robust fine-tuning of zero-shot models](https://openaccess.thecvf.com/content/CVPR2022/papers/Wortsman_Robust_Fine-Tuning_of_Zero-Shot_Models_CVPR_2022_paper.pdf) | University of Washington | [Link](https://github.com/mlfoundations/wise-ft) |
| 2022 | IJCV | [Learning to Prompt for Vision-Language Models](https://arxiv.org/abs/2109.01134) | Nanyang Technological University | [Link](https://github.com/KaiyangZhou/CoOp) |
| 2022 | NeurIPS | [Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models](https://arxiv.org/abs/2209.07511) | Nvidia | [Link](https://azshue.github.io/TPT/) |
</details>

<details><summary>Vision-Language Model - Zero Shot Learning</summary>

## Vision-Language Model - Zero Shot Learning
| Year | Venue | Title | Institute | Code |
| :---:| :---: | :---: | :---: | :---: |
| 2023 | AAAI | [CALIP: Zero-Shot Enhancement of CLIP with Parameter-free Attention](https://arxiv.org/abs/2209.14169) |  Peking University | [Link](https://github.com/ZiyuGuo99/CALIP) |
| 2023 | CVPR | [Improving Zero-shot Generalization and Robustness of Multi-modal Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Ge_Improving_Zero-Shot_Generalization_and_Robustness_of_Multi-Modal_Models_CVPR_2023_paper.pdf) | Google Research | [Link](https://github.com/gyhandy/Hierarchy-CLIP) |
| 2023 | CVPR | [Texts as Images in Prompt Tuning for Multi-Label Image Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Texts_as_Images_in_Prompt_Tuning_for_Multi-Label_Image_Recognition_CVPR_2023_paper.pdf) | Harbin Institute of Technology | [Link](https://github.com/guozix/TaI-DPT) |
| 2023 | ICCV | [SuS-X: Training-Free Name-Only Transfer of Vision-Language Models](https://arxiv.org/abs/2211.16198) | University of Cambridge | [Link](https://github.com/vishaal27/SuS-X) |
| 2023 | ICCV | [What does a platypus look like? Generating customized prompts for zero-shot image classification](https://openaccess.thecvf.com/content/ICCV2023/papers/Pratt_What_Does_a_Platypus_Look_Like_Generating_Customized_Prompts_for_ICCV_2023_paper.pdf) | University of Washington | [Link](https://github.com/sarahpratt/CuPL) |
| 2023 | ICML | [CHiLS: Zero-Shot Image Classification with Hierarchical Label Sets](https://proceedings.mlr.press/v202/novack23a/novack23a.pdf) | University of California, San Diego | [Link](https://github.com/acmi-lab/CHILS) |
| 2022 | BMVC | [SVL-Adapter: Self-Supervised Adapter for Vision-Language Pretrained Models](https://arxiv.org/pdf/2210.03794) | University College London | [Link](https://github.com/omipan/svl_adapter) |
| 2022 | NeurIPS | [Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models](https://arxiv.org/abs/2209.07511) | Nvidia | [Link](https://azshue.github.io/TPT/) |
</details>

<details> <summary>Vision-Language Model - Few Shot Learning</summary>

## Vision-Language Model - Few Shot Learning
| Year | Venue | Title | Institute | Code |
| :---:| :---: | :---: | :---: | :---: |
| 2023 | AAAI | [CALIP: Zero-Shot Enhancement of CLIP with Parameter-free Attention](https://arxiv.org/abs/2209.14169) | Peking University | [Link](https://github.com/ZiyuGuo99/CALIP) |
| 2023 | Arxiv | [Language Models as Black-box Optimizers for Vision-language Models](https://arxiv.org/abs/2309.05950) | Carnegie Mellon University | - |
| 2023 | CVPR | [LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Bulat_LASP_Text-to-Text_Optimization_for_Language-Aware_Soft_Prompting_of_Vision__CVPR_2023_paper.pdf) | Samsung AI Cambridge | - |
| 2023 | CVPR | [MaPLe: Multi-modal Prompt Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Khattak_MaPLe_Multi-Modal_Prompt_Learning_CVPR_2023_paper.pdf) | Mohamed bin Zayed University of AI | [Link](https://github.com/muzairkhattak/multimodal-prompt-learning) |
| 2023 | CVPR | [Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners](https://arxiv.org/abs/2303.02151) | Shanghai AI Laboratory | [Link](https://github.com/ZrrSkywalker/CaFo) |
| 2023 | CVPR | [Texts as Images in Prompt Tuning for Multi-Label Image Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Texts_as_Images_in_Prompt_Tuning_for_Multi-Label_Image_Recognition_CVPR_2023_paper.pdf) | Harbin Institute of Technology | [Link](https://github.com/guozix/TaI-DPT) |
| 2023 | CVPR | [Task Residual for Tuning Vision-Language Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Task_Residual_for_Tuning_Vision-Language_Models_CVPR_2023_paper.pdf) | National University of Singapore | [Link](https://github.com/geekyutao/TaskRes) |
| 2023 | CVPR | [Visual-Language Prompt Tuning with Knowledge-guided Context Optimization](https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_Visual-Language_Prompt_Tuning_With_Knowledge-Guided_Context_Optimization_CVPR_2023_paper.pdf) | Chinese Academy of Sciences | [Link](https://github.com/htyao89/KgCoOp) |
| 2023 | ICCV | [Bayesian Prompt Learning for Image-Language Model Generalization](https://openaccess.thecvf.com/content/ICCV2023/papers/Derakhshani_Bayesian_Prompt_Learning_for_Image-Language_Model_Generalization_ICCV_2023_paper.pdf) | University of Amsterdam | [Link](https://github.com/saic-fi/Bayesian-Prompt-Learning) |
| 2023 | ICCV | [Black Box Few-Shot Adaptation for Vision-Language models](https://openaccess.thecvf.com/content/ICCV2023/papers/Ouali_Black_Box_Few-Shot_Adaptation_for_Vision-Language_Models_ICCV_2023_paper.pdf) | Samsung AI Cambridge | [Link](https://github.com/saic-fi/LFA) |
| 2023 | ICCV | [Distribution-Aware Prompt Tuning for Vision-Language Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Cho_Distribution-Aware_Prompt_Tuning_for_Vision-Language_Models_ICCV_2023_paper.pdf) | Korea University | [Link](https://github.com/mlvlab/DAPT) |
| 2023 | ICCV | [Gradient-Regulated Meta-Prompt Learning for Generalizable Vision-Language Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Gradient-Regulated_Meta-Prompt_Learning_for_Generalizable_Vision-Language_Models_ICCV_2023_paper.pdf) | Zhejiang University | - |
| 2023 | ICCV | [Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Kan_Knowledge-Aware_Prompt_Tuning_for_Generalizable_Vision-Language_Models_ICCV_2023_paper.pdf) | Qilu University of Technology | - |
| 2023 | ICCV | [Not All Features Matter: Enhancing Few-shot CLIP with Adaptive Prior Refinement](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Not_All_Features_Matter_Enhancing_Few-shot_CLIP_with_Adaptive_Prior_ICCV_2023_paper.pdf) | Shanghai AI Laboratory | [Link](https://github.com/yangyangyang127/APE) |
| 2023 | ICCV | [Prompt-aligned Gradient for Prompt Tuning](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Prompt-aligned_Gradient_for_Prompt_Tuning_ICCV_2023_paper.pdf) | Nanyang Technological University | [Link](https://github.com/BeierZhu/Prompt-align) |
| 2023 | ICCV | [Read-only Prompt Optimization for Vision-Language Few-shot Learning](https://openaccess.thecvf.com//content/ICCV2023/papers/Lee_Read-only_Prompt_Optimization_for_Vision-Language_Few-shot_Learning_ICCV_2023_paper.pdf) | Korea University | [Link](https://github.com/mlvlab/RPO) |
| 2023 | ICCV | [Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-trained Vision-Language Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Zheng_Regularized_Mask_Tuning_Uncovering_Hidden_Knowledge_in_Pre-Trained_Vision-Language_Models_ICCV_2023_paper.pdf) | Zhejiang University | [Link](https://github.com/wuw2019/R-AMT) |
| 2023 | ICCV | [SuS-X: Training-Free Name-Only Transfer of Vision-Language Models](https://arxiv.org/abs/2211.16198) | University of Cambridge | [Link](https://github.com/vishaal27/SuS-X) |
| 2023 | ICCV | [Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Why_Is_Prompt_Tuning_for_Vision-Language_Models_Robust_to_Noisy_ICCV_2023_paper.pdf) | University of Wisconsin, Madison | [Link](https://github.com/CEWu/PTNL) |
| 2023 | ICLR | [Exploiting category names for Few-shot Classification with Vision-language Models](https://arxiv.org/abs/2211.16594) | University of California, Merced  | - |
| 2023 | ICLR | [PLOT: Prompt Learning with Optimal Transport for Vision-Language Models](https://arxiv.org/abs/2210.01253) | Carnegie Mellon University | [Link](https://github.com/CHENGY12/PLOT) |
| 2022 | AAAI | [Revisiting Few-Shot Learning from a Causal Perspective](https://arxiv.org/abs/2209.13816) | Sun Yat-Sen university | [Link](https://github.com/lingl1024/causalFewShot) |
| 2022 | BMVC | [SVL-Adapter: Self-Supervised Adapter for Vision-Language Pretrained Models](https://arxiv.org/pdf/2210.03794) | University College London | [Link](https://github.com/omipan/svl_adapter) |
| 2022 | CVPR | [Conditional Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2203.05557) | Nanyang Technological University | [Link](https://github.com/KaiyangZhou/CoOp) |
| 2022 | CVPR | [DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting](https://openaccess.thecvf.com/content/CVPR2022/papers/Rao_DenseCLIP_Language-Guided_Dense_Prediction_With_Context-Aware_Prompting_CVPR_2022_paper.pdf) | Tsinghua University | [Link](https://github.com/raoyongming/DenseCLIP) |
| 2022 | CVPR | [Prompt Distribution Learning](https://openaccess.thecvf.com/content/CVPR2022/papers/Lu_Prompt_Distribution_Learning_CVPR_2022_paper.pdf) | University of Science and Technology of China | - |
| 2022 | ECCV | [Tip-Adapter: Training-free Adaption of CLIP for Few-shot Classification](https://arxiv.org/abs/2111.03930) | Shanghai AI Laboratory | [Link](https://github.com/gaopengcuhk/Tip-Adapter) |
| 2022 | EMNLP | [CPL: Counterfactual Prompt Learning for Vision and Language Models](https://arxiv.org/abs/2210.10362) | University of California, Santa Cruz | [Link](https://github.com/eric-ai-lab/CPL) |
| 2022 | IJCV | [Learning to Prompt for Vision-Language Models](https://arxiv.org/abs/2109.01134) | Nanyang Technological University | [Link](https://github.com/KaiyangZhou/CoOp) |
| 2022 | NeurIPS | [DualCoOp: Fast Adaptation to Multi-Label Recognition with Limited Annotations](https://arxiv.org/abs/2206.09541) | Boston University | [Link](https://github.com/sunxm2357/DualCoOp) |
| 2021 | Arxiv | [CLIP-Adapter: Better Vision-Language Models with Feature Adapters](https://arxiv.org/abs/2110.04544) | Shanghai AI Laboratory | [Link](https://github.com/gaopengcuhk/CLIP-Adapter) |
| 2021 | Arxiv | [VT-CLIP: Enhancing Vision-Language Models with Visual-guided Texts](https://arxiv.org/abs/2112.02399) | ShanghaiTech University | - |
</details>

<details><summary>Vision-Language Model - Other Downstream</summary>

## Vision-Language Model - Other Downstream
| Year | Venue | Title | Institute | Code |
| :---:| :---: | :---: | :---: | :---: |
| 2023 | Arxiv | [UP-DP: Unsupervised Prompt Learning for Data Pre-Selection with Vision-Language Models](https://arxiv.org/abs/2307.11227) | Bosch Research | - |
| 2023 | AAAI | [CLIP-ReID: Exploiting Vision-Language Model for Image Re-identification without Concrete Text Labels](https://arxiv.org/abs/2211.13977) | East China Normal University | [Link](https://github.com/Syliz517/CLIP-ReID)
| 2023 | IJCV |  [Exploring Vision-Language Models for Imbalanced Learning](https://arxiv.org/abs/2304.01457) | Peking University | [Link](https://github.com/Imbalance-VLM/Imbalance-VLM)
| 2022 | Arxiv | [Domain Adaptation via Prompt Learning](https://arxiv.org/abs/2202.06687) | Tsinghua University | [Link](https://github.com/LeapLabTHU/DAPrompt)
| 2022 | Arxiv | [Unsupervised Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2204.03649) | Peking University | [Link](https://github.com/tonyhuang2022/UPL) |
| 2022 | CVPR | [CLIPstyler: Image Style Transfer with a Single Text Condition](https://arxiv.org/abs/2112.00374) | KAIST | [Link](https://github.com/cyclomon/CLIPstyler) |
| 2022 | ECCV | [Extract Free Dense Labels from CLIP](https://arxiv.org/abs/2112.01071) | Nanyang Technological University | [Link](https://github.com/chongzhou96/MaskCLIP)
| 2022 | ECCV | [Prompting Visual-Language Models for Efficient Video Understanding](https://arxiv.org/abs/2112.04478) | Shanghai Jiao Tong University | [Link](https://github.com/ju-chen/Efficient-Prompt)
| 2022 | ECCV | [Visual Prompt Tuning](https://arxiv.org/abs/2203.12119) | Cornell University | [Link](https://github.com/kmnp/vpt)
| 2022 | ICLR | [Open-vocabulary Object Detection via Vision and Language Knowledge Distillation](https://arxiv.org/abs/2104.13921) | Google Research | [Link](https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild)
| 2021 | ICCV | [StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery](https://arxiv.org/abs/2103.17249) | Hebrew University of Jerusalem | [Link](https://github.com/orpatashnik/StyleCLIP)
</details>