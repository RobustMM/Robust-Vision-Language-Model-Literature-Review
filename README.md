# Robust-Vision-Language-Model-Literature-Review

## Vision-Language Foundation Model
| Year | Venue | Title | Institute | Code |
| :---:| :---: | :---: | :---: | :---: |
| 2023 | Arxiv | [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) | Salesforce Research | [Link](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)
| 2023 | Arxiv | [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500) | Salesforce Research | [Link](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)
| 2023 | CVPR | [Image as a Foreign Language: BEIT Pretraining for Vision and Vision-Language Tasks](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Image_as_a_Foreign_Language_BEiT_Pretraining_for_Vision_and_CVPR_2023_paper.pdf) | Microsoft Corporation| [Link](https://github.com/microsoft/unilm/tree/master/beit3)
| 2023 | ICCV | [ALIP: Adaptive Language-Image Pre-training with Synthetic Caption](https://arxiv.org/abs/2308.08428) | DeepGlint | [Link](https://github.com/deepglint/ALIP)
| 2023 | ICCV | [GrowCLIP: Data-aware Automatic Model Growing for Large-scale Contrastive Language-Image Pre-training](https://openaccess.thecvf.com/content/ICCV2023/papers/Deng_GrowCLIP_Data-Aware_Automatic_Model_Growing_for_Large-scale_Contrastive_Language-Image_Pre-Training_ICCV_2023_paper.pdf) | Sun Yat-sen University  | \
| 2023 | ICLR | [PaLI: A Jointly-Scaled Multilingual Language-Image Model](https://openreview.net/forum?id=mWVoBz4W0u) | Google Research | [Link](https://github.com/kyegomez/PALI)
| 2022 | ICML  | [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://proceedings.mlr.press/v162/li22n/li22n.pdf)  | Salesforce Research | [Link](https://github.com/salesforce/BLIP)
| 2021 | ICML  | [Learning Transferable Visual Models From Natural Language Supervision](https://proceedings.mlr.press/v139/radford21a/radford21a.pdf) | OpenAI | [Link](https://github.com/OpenAI/CLIP)


## Vision-Language Model - Domain Generalization
| Year | Venue | Title | Institute | Code |
| :---:| :---: | :---: | :---: | :---: |
| 2023 | ICCV | [Bayesian Prompt Learning for Image-Language Model Generalization](https://openaccess.thecvf.com/content/ICCV2023/papers/Derakhshani_Bayesian_Prompt_Learning_for_Image-Language_Model_Generalization_ICCV_2023_paper.pdf) | University of Amsterdam | [link](https://github.com/saic-fi/Bayesian-Prompt-Learning) |
| 2023 | ICCV | [Black Box Few-Shot Adaptation for Vision-Language models](https://openaccess.thecvf.com/content/ICCV2023/papers/Ouali_Black_Box_Few-Shot_Adaptation_for_Vision-Language_Models_ICCV_2023_paper.pdf) | Samsung AI Cambridge | [link](https://github.com/saic-fi/LFA) |
| 2023 | ICCV | [Distribution-Aware Prompt Tuning for Vision-Language Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Cho_Distribution-Aware_Prompt_Tuning_for_Vision-Language_Models_ICCV_2023_paper.pdf) | Korea University | [link](https://github.com/mlvlab/DAPT) |
| 2023 | ICCV | [Gradient-Regulated Meta-Prompt Learning for Generalizable Vision-Language Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Gradient-Regulated_Meta-Prompt_Learning_for_Generalizable_Vision-Language_Models_ICCV_2023_paper.pdf) | Zhejiang University | \ |
| 2023 | ICCV | [Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Kan_Knowledge-Aware_Prompt_Tuning_for_Generalizable_Vision-Language_Models_ICCV_2023_paper.pdf) | Qilu University of Technology | \ |
| 2023 | ICCV | [LoGoPrompt: Synthetic Text Images Can Be Good Visual Prompts for Vision-Language Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Shi_LoGoPrompt_Synthetic_Text_Images_Can_Be_Good_Visual_Prompts_for_ICCV_2023_paper.pdf) | ShanghaiTech University | \ |
| 2023 | ICCV | [PADCLIP: Pseudo-labeling with Adaptive Debiasing in CLIP for Unsupervised Domain Adaptation](https://openaccess.thecvf.com/content/ICCV2023/papers/Lai_PADCLIP_Pseudo-labeling_with_Adaptive_Debiasing_in_CLIP_for_Unsupervised_Domain_ICCV_2023_paper.pdf) | University of California, Davis | \ |
| 2023 | ICCV | [PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization](https://openaccess.thecvf.com/content/ICCV2023/papers/Cho_PromptStyler_Prompt-driven_Style_Generation_for_Source-free_Domain_Generalization_ICCV_2023_paper.pdf) | ADD | \ |
| 2023 | ICCV | [Read-only Prompt Optimization for Vision-Language Few-shot Learning](https://openaccess.thecvf.com//content/ICCV2023/papers/Lee_Read-only_Prompt_Optimization_for_Vision-Language_Few-shot_Learning_ICCV_2023_paper.pdf) | Korea University | [link](https://github.com/mlvlab/RPO) |
| 2023 | ICCV | [Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-trained Vision-Language Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Zheng_Regularized_Mask_Tuning_Uncovering_Hidden_Knowledge_in_Pre-Trained_Vision-Language_Models_ICCV_2023_paper.pdf) | Zhejiang University | [link](https://github.com/wuw2019/R-AMT) |
| 2023 | ICCVW | [AD-CLIP: Adapting Domains in Prompt Space Using CLIP](https://arxiv.org/abs/2308.05659) | Indian Institute of Technology Bombay | \ |
| 2022 | CVPR | [Conditional Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2203.05557) | Nanyang Technological University | [link](https://github.com/KaiyangZhou/CoOp) |
| 2022 | IJCV | [Learning to Prompt for Vision-Language Models](https://arxiv.org/abs/2109.01134) | Nanyang Technological University | [link](https://github.com/KaiyangZhou/CoOp) |
| 2022 | NeurIPS | [Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models](https://arxiv.org/abs/2209.07511) | Nvidia | [link](https://azshue.github.io/TPT/) |

## Vision-Language Model - Zero Shot Learning
| Year | Venue | Title | Institute | Code |
| :---:| :---: | :---: | :---: | :---: |
| 2023 | AAAI | [CALIP: Zero-Shot Enhancement of CLIP with Parameter-free Attention](https://arxiv.org/abs/2209.14169) |  Peking University | [link](https://github.com/ZiyuGuo99/CALIP) |
| 2023 | ICCV | [SuS-X: Training-Free Name-Only Transfer of Vision-Language Models](https://arxiv.org/abs/2211.16198) | University of Cambridge | [link](https://github.com/vishaal27/SuS-X) |
| 2022 | BMVC | [SVL-Adapter: Self-Supervised Adapter for Vision-Language Pretrained Models](https://arxiv.org/pdf/2210.03794) | University College London | [link](https://github.com/omipan/svl_adapter) |
| 2022 | NeurIPS | [Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models](https://arxiv.org/abs/2209.07511) | Nvidia | [link](https://azshue.github.io/TPT/) |


## Vision-Language Model - Few Shot Learning
| Year | Venue | Title | Institute | Code |
| :---:| :---: | :---: | :---: | :---: |
| 2023 | AAAI | [CALIP: Zero-Shot Enhancement of CLIP with Parameter-free Attention](https://arxiv.org/abs/2209.14169) | Peking University | [link](https://github.com/ZiyuGuo99/CALIP) |
| 2023 | Arxiv | [Language Models as Black-box Optimizers for Vision-language Models](https://arxiv.org/abs/2309.05950) | Carnegie Mellon University | \ |
| 2023 | CVPR | [Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners](https://arxiv.org/abs/2303.02151) | Shanghai AI Laboratory | [link](https://github.com/ZrrSkywalker/CaFo) |
| 2023 | ICCV | [Bayesian Prompt Learning for Image-Language Model Generalization](https://openaccess.thecvf.com/content/ICCV2023/papers/Derakhshani_Bayesian_Prompt_Learning_for_Image-Language_Model_Generalization_ICCV_2023_paper.pdf) | University of Amsterdam | [link](https://github.com/saic-fi/Bayesian-Prompt-Learning) |
| 2023 | ICCV | [Black Box Few-Shot Adaptation for Vision-Language models](https://openaccess.thecvf.com/content/ICCV2023/papers/Ouali_Black_Box_Few-Shot_Adaptation_for_Vision-Language_Models_ICCV_2023_paper.pdf) | Samsung AI Cambridge | [link](https://github.com/saic-fi/LFA) |
| 2023 | ICCV | [Distribution-Aware Prompt Tuning for Vision-Language Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Cho_Distribution-Aware_Prompt_Tuning_for_Vision-Language_Models_ICCV_2023_paper.pdf) | Korea University | [link](https://github.com/mlvlab/DAPT) |
| 2023 | ICCV | [Gradient-Regulated Meta-Prompt Learning for Generalizable Vision-Language Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Gradient-Regulated_Meta-Prompt_Learning_for_Generalizable_Vision-Language_Models_ICCV_2023_paper.pdf) | Zhejiang University | \ |
| 2023 | ICCV | [Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Kan_Knowledge-Aware_Prompt_Tuning_for_Generalizable_Vision-Language_Models_ICCV_2023_paper.pdf) | Qilu University of Technology | \ |
| 2023 | ICCV | [Not All Features Matter: Enhancing Few-shot CLIP with Adaptive Prior Refinement](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Not_All_Features_Matter_Enhancing_Few-shot_CLIP_with_Adaptive_Prior_ICCV_2023_paper.pdf) | Shanghai AI Laboratory | [link](https://github.com/yangyangyang127/APE) |
| 2023 | ICCV | [Read-only Prompt Optimization for Vision-Language Few-shot Learning](https://openaccess.thecvf.com//content/ICCV2023/papers/Lee_Read-only_Prompt_Optimization_for_Vision-Language_Few-shot_Learning_ICCV_2023_paper.pdf) | Korea University | [link](https://github.com/mlvlab/RPO) |
| 2023 | ICCV | [Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-trained Vision-Language Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Zheng_Regularized_Mask_Tuning_Uncovering_Hidden_Knowledge_in_Pre-Trained_Vision-Language_Models_ICCV_2023_paper.pdf) | Zhejiang University | [link](https://github.com/wuw2019/R-AMT) |
| 2023 | ICCV | [SuS-X: Training-Free Name-Only Transfer of Vision-Language Models](https://arxiv.org/abs/2211.16198) | University of Cambridge | [link](https://github.com/vishaal27/SuS-X) |
| 2023 | ICCV | [Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Why_Is_Prompt_Tuning_for_Vision-Language_Models_Robust_to_Noisy_ICCV_2023_paper.pdf) | University of Wisconsin-Madison | [link](https://github.com/CEWu/PTNL) |
| 2022 | AAAI | [Revisiting Few-Shot Learning from a Causal Perspective](https://arxiv.org/abs/2209.13816) | Sun Yat-Sen university | [link](https://github.com/lingl1024/causalFewShot) |
| 2022 | BMVC | [SVL-Adapter: Self-Supervised Adapter for Vision-Language Pretrained Models](https://arxiv.org/pdf/2210.03794) | University College London | [link](https://github.com/omipan/svl_adapter) |
| 2022 | CVPR | [Conditional Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2203.05557) | Nanyang Technological University | [link](https://github.com/KaiyangZhou/CoOp) |
| 2022 | ECCV | [Tip-Adapter: Training-free Adaption of CLIP for Few-shot Classification](https://arxiv.org/abs/2111.03930) | Shanghai AI Laboratory | [link](https://github.com/gaopengcuhk/Tip-Adapter) |
| 2022 | IJCV | [Learning to Prompt for Vision-Language Models](https://arxiv.org/abs/2109.01134) | Nanyang Technological University | [link](https://github.com/KaiyangZhou/CoOp) |
| 2021 | Arxiv | [CLIP-Adapter: Better Vision-Language Models with Feature Adapters](https://arxiv.org/abs/2110.04544) | Shanghai AI Laboratory | [link](https://github.com/gaopengcuhk/CLIP-Adapter) |
| 2021 | Arxiv | [VT-CLIP: Enhancing Vision-Language Models with Visual-guided Texts](https://arxiv.org/abs/2112.02399) | ShanghaiTech University | \ |

## Vision-Language Model - Other Downstream
| Year | Venue | Title | Institute | Code |
| :---:| :---: | :---: | :---: | :---: |
| 2023 | Arxiv | [UP-DP: Unsupervised Prompt Learning for Data Pre-Selection with Vision-Language Models](https://arxiv.org/abs/2307.11227) | Bosch Research | \ |
| 2022 | Arxiv | [Unsupervised Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2204.03649) | Peking University | [link](https://github.com/tonyhuang2022/UPL) |