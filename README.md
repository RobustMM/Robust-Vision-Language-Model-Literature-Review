# Robust-Vision-Language-Model-Literature-Review

## Vision-Language Foundation Model
| Year | Venue | Title | Institute | Code |
| :---:| :---: | :---: | :---: | :---: |
| 2023 | Arxiv | [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) | Salesforce Research | [Link](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)
| 2023 | Arxiv | [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500) | Salesforce Research | [Link](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)
| 2023 | CVPR | [Image as a Foreign Language: BEIT Pretraining for Vision and Vision-Language Tasks](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Image_as_a_Foreign_Language_BEiT_Pretraining_for_Vision_and_CVPR_2023_paper.pdf) | Microsoft | [Link](https://github.com/microsoft/unilm/tree/master/beit3)
| 2023 | ICLR | [PaLI: A Jointly-Scaled Multilingual Language-Image Model](https://openreview.net/forum?id=mWVoBz4W0u) | Google Research | [Link](https://github.com/kyegomez/PALI)
| 2022 | ICML  | [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://proceedings.mlr.press/v162/li22n/li22n.pdf)  | Salesforce Research | [Link](https://github.com/salesforce/BLIP)
| 2021 | ICML  | [Learning Transferable Visual Models From Natural Language Supervision](https://proceedings.mlr.press/v139/radford21a/radford21a.pdf) | OpenAI | [Link](https://github.com/OpenAI/CLIP)


## Vision-Language Model - Domain Generalization
| Year | Venue | Title | Institute | Code |
| :---:| :---: | :---: | :---: | :---: |
| 2023 | AAAI | [CALIP: Zero-Shot Enhancement of CLIP with Parameter-free Attention](https://arxiv.org/abs/2209.14169) |  Peking University | [link](https://github.com/ZiyuGuo99/CALIP) |
| 2023 | ICCV | [SuS-X: Training-Free Name-Only Transfer of Vision-Language Models](https://arxiv.org/abs/2211.16198) | Shanghai AI Laboratory | [link](https://github.com/vishaal27/SuS-X) |
| 2022 | CVPR | [Conditional Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2203.05557) | Nanyang Technological University | [link](https://github.com/KaiyangZhou/CoOp) |
| 2022 | IJCV | [Learning to Prompt for Vision-Language Models](https://arxiv.org/abs/2109.01134) | Nanyang Technological University | [link](https://github.com/KaiyangZhou/CoOp) |
| 2022 | NeurIPS | [Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models](https://arxiv.org/abs/2209.07511) | Nvidia | [link](https://azshue.github.io/TPT/) |


## Vision-Language Model - Few Shot Learning
| Year | Venue | Title | Institute | Code |
| :---:| :---: | :---: | :---: | :---: |
| 2023 | CVPR | [Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners](https://arxiv.org/abs/2303.02151) | Shanghai AI Laboratory | [link](https://github.com/ZrrSkywalker/CaFo) |
| 2023 | ICCV | [Not All Features Matter: Enhancing Few-shot CLIP with Adaptive Prior Refinement](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Not_All_Features_Matter_Enhancing_Few-shot_CLIP_with_Adaptive_Prior_ICCV_2023_paper.pdf) | Shanghai AI Laboratory | [link](https://github.com/yangyangyang127/APE) |
| 2022 | AAAI | [Revisiting Few-Shot Learning from a Causal Perspective](https://arxiv.org/abs/2209.13816) | Sun Yat-Sen university | [link](https://github.com/lingl1024/causalFewShot) |
| 2022 | CVPR | [Conditional Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2203.05557) | Nanyang Technological University | [link](https://github.com/KaiyangZhou/CoOp) |
| 2022 | ECCV | [Tip-Adapter: Training-free Adaption of CLIP for Few-shot Classification](https://arxiv.org/abs/2111.03930) | Shanghai AI Laboratory | [link](https://github.com/gaopengcuhk/Tip-Adapter) |
| 2022 | IJCV | [Learning to Prompt for Vision-Language Models](https://arxiv.org/abs/2109.01134) | Nanyang Technological University | [link](https://github.com/KaiyangZhou/CoOp) |
| 2021 | Arxiv | [CLIP-Adapter: Better Vision-Language Models with Feature Adapters](https://arxiv.org/abs/2110.04544) | Shanghai AI Laboratory | [link](https://github.com/gaopengcuhk/CLIP-Adapter) |

## Vision-Language Model - Other Downstream
| Year | Venue | Title | Institute | Code |
| :---:| :---: | :---: | :---: | :---: |
