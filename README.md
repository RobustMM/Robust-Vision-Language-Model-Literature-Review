# Robust-Vision-Language-Model-Literature-Review

## Vision-Language Foundation Model
| Year | Venue | Title | Institute | Code |
| :---:| :---: | :---: | :---: | :---: |
| 2023 | Arxiv | [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) | Salesforce Research | [Link](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)
| 2023 | Arxiv | [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500) | Salesforce Research | [Link](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)
| 2023 | CVPR | [Image as a Foreign Language: BEIT Pretraining for Vision and Vision-Language Tasks](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Image_as_a_Foreign_Language_BEiT_Pretraining_for_Vision_and_CVPR_2023_paper.pdf) | Microsoft Corporation| [Link](https://github.com/microsoft/unilm/tree/master/beit3)
| 2023 | ICCV | [ALIP: Adaptive Language-Image Pre-training with Synthetic Caption](https://arxiv.org/abs/2308.08428) | DeepGlint | [Link](https://github.com/deepglint/ALIP)
| 2023 | ICCV | [GrowCLIP: Data-aware Automatic Model Growing for Large-scale Contrastive Language-Image Pre-training](https://openaccess.thecvf.com/content/ICCV2023/papers/Deng_GrowCLIP_Data-Aware_Automatic_Model_Growing_for_Large-scale_Contrastive_Language-Image_Pre-Training_ICCV_2023_paper.pdf) | Sun Yat-sen University  | \
| 2023 | ICLR | [PaLI: A Jointly-Scaled Multilingual Language-Image Model](https://openreview.net/forum?id=mWVoBz4W0u) | Google Research | [Link](https://github.com/kyegomez/PALI)
| 2022 | ICML  | [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://proceedings.mlr.press/v162/li22n/li22n.pdf)  | Salesforce Research | [Link](https://github.com/salesforce/BLIP)
| 2021 | ICML  | [Learning Transferable Visual Models From Natural Language Supervision](https://proceedings.mlr.press/v139/radford21a/radford21a.pdf) | OpenAI | [Link](https://github.com/OpenAI/CLIP)


## Vision-Language Model - Domain Generalization
| Year | Venue | Title | Institute | Code |
| :---:| :---: | :---: | :---: | :---: |
| 2023 | ICCV | [Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-trained Vision-Language Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Zheng_Regularized_Mask_Tuning_Uncovering_Hidden_Knowledge_in_Pre-Trained_Vision-Language_Models_ICCV_2023_paper.pdf) | Zhejiang University | [link](https://github.com/wuw2019/R-AMT) |
| 2022 | CVPR | [Conditional Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2203.05557) | Nanyang Technological University | [link](https://github.com/KaiyangZhou/CoOp) |
| 2022 | IJCV | [Learning to Prompt for Vision-Language Models](https://arxiv.org/abs/2109.01134) | Nanyang Technological University | [link](https://github.com/KaiyangZhou/CoOp) |
| 2022 | NeurIPS | [Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models](https://arxiv.org/abs/2209.07511) | Nvidia | [link](https://azshue.github.io/TPT/) |

## Vision-Language Model - Zero Shot Learning
| Year | Venue | Title | Institute | Code |
| :---:| :---: | :---: | :---: | :---: |
| 2023 | AAAI | [CALIP: Zero-Shot Enhancement of CLIP with Parameter-free Attention](https://arxiv.org/abs/2209.14169) |  Peking University | [link](https://github.com/ZiyuGuo99/CALIP) |
| 2023 | ICCV | [SuS-X: Training-Free Name-Only Transfer of Vision-Language Models](https://arxiv.org/abs/2211.16198) | University of Cambridge | [link](https://github.com/vishaal27/SuS-X) |
| 2022 | BMVC | [SVL-Adapter: Self-Supervised Adapter for Vision-Language Pretrained Models](https://arxiv.org/pdf/2210.03794) | University College London | [link](https://github.com/omipan/svl_adapter) |
| 2022 | NeurIPS | [Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models](https://arxiv.org/abs/2209.07511) | Nvidia | [link](https://azshue.github.io/TPT/) |


## Vision-Language Model - Few Shot Learning
| Year | Venue | Title | Institute | Code |
| :---:| :---: | :---: | :---: | :---: |
| 2023 | AAAI | [CALIP: Zero-Shot Enhancement of CLIP with Parameter-free Attention](https://arxiv.org/abs/2209.14169) | Peking University | [link](https://github.com/ZiyuGuo99/CALIP) |
| 2023 | CVPR | [Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners](https://arxiv.org/abs/2303.02151) | Shanghai AI Laboratory | [link](https://github.com/ZrrSkywalker/CaFo) |
| 2023 | ICCV | [Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Why_Is_Prompt_Tuning_for_Vision-Language_Models_Robust_to_Noisy_ICCV_2023_paper.pdf) | University of Wisconsin-Madison | [link](https://github.com/CEWu/PTNL) |
| 2023 | ICCV | [Not All Features Matter: Enhancing Few-shot CLIP with Adaptive Prior Refinement](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Not_All_Features_Matter_Enhancing_Few-shot_CLIP_with_Adaptive_Prior_ICCV_2023_paper.pdf) | Shanghai AI Laboratory | [link](https://github.com/yangyangyang127/APE) |
| 2023 | ICCV | [Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-trained Vision-Language Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Zheng_Regularized_Mask_Tuning_Uncovering_Hidden_Knowledge_in_Pre-Trained_Vision-Language_Models_ICCV_2023_paper.pdf) | Zhejiang University | [link](https://github.com/wuw2019/R-AMT) |
| 2023 | ICCV | [SuS-X: Training-Free Name-Only Transfer of Vision-Language Models](https://arxiv.org/abs/2211.16198) | University of Cambridge | [link](https://github.com/vishaal27/SuS-X) |
| 2022 | AAAI | [Revisiting Few-Shot Learning from a Causal Perspective](https://arxiv.org/abs/2209.13816) | Sun Yat-Sen university | [link](https://github.com/lingl1024/causalFewShot) |
| 2022 | BMVC | [SVL-Adapter: Self-Supervised Adapter for Vision-Language Pretrained Models](https://arxiv.org/pdf/2210.03794) | University College London | [link](https://github.com/omipan/svl_adapter) |
| 2022 | CVPR | [Conditional Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2203.05557) | Nanyang Technological University | [link](https://github.com/KaiyangZhou/CoOp) |
| 2022 | ECCV | [Tip-Adapter: Training-free Adaption of CLIP for Few-shot Classification](https://arxiv.org/abs/2111.03930) | Shanghai AI Laboratory | [link](https://github.com/gaopengcuhk/Tip-Adapter) |
| 2022 | IJCV | [Learning to Prompt for Vision-Language Models](https://arxiv.org/abs/2109.01134) | Nanyang Technological University | [link](https://github.com/KaiyangZhou/CoOp) |
| 2021 | Arxiv | [CLIP-Adapter: Better Vision-Language Models with Feature Adapters](https://arxiv.org/abs/2110.04544) | Shanghai AI Laboratory | [link](https://github.com/gaopengcuhk/CLIP-Adapter) |
| 2021 | Arxiv | [VT-CLIP: Enhancing Vision-Language Models with Visual-guided Texts](https://arxiv.org/abs/2112.02399) | ShanghaiTech University | \ |

## Vision-Language Model - Other Downstream
| Year | Venue | Title | Institute | Code |
| :---:| :---: | :---: | :---: | :---: |
| 2023 | Arxiv | [UP-DP: Unsupervised Prompt Learning for Data Pre-Selection with Vision-Language Models](https://arxiv.org/abs/2307.11227) | Bosch Research | \ |
| 2022 | Arxiv | [Unsupervised Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2204.03649) | Peking University | [link](https://github.com/tonyhuang2022/UPL) |